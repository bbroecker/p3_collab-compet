{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "from madddpg_utils import DDPGConfig, MADDDPGConfig, train_agent, EnvironmentWrapper, evaluate_current_weights, \\\n",
    "    ExploreStrategy\n",
    "from maddpg_trainer import MADDPGManager\n",
    "from maddpg_tennis import default_cfg\n",
    "\n",
    "NUM_SUB_POLICIES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name='environments/Tennis_Linux/Tennis.x86_64')\n",
    "env = EnvironmentWrapper(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Init Agent\n",
    "\n",
    "Initialize a MADDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_cfg = []\n",
    "config = default_cfg()\n",
    "config.multi_agent_actions = env.action_sizes\n",
    "config.multi_agent_states = env.state_sizes\n",
    "for i in range(env.num_agents):\n",
    "    agent_cfg = copy.deepcopy(config)\n",
    "    agent_cfg.state_size = config.multi_agent_states[i]\n",
    "    agent_cfg.action_size = config.multi_agent_actions[i]\n",
    "    maddpg_cfg = MADDDPGConfig()\n",
    "    maddpg_cfg.subpolicy_configs = [agent_cfg] * NUM_SUB_POLICIES\n",
    "    multi_cfg.append(maddpg_cfg)\n",
    "\n",
    "agent = MADDPGManager(maddpg_agents_configs=multi_cfg, update_every=config.update_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train your agent\n",
    "\n",
    "This following section will train a new agent, the weights are only saved when the agent reaches an average of 0.5+ points. If you want to test the pre-trained weights skip to Section 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dir = 'new_weights/'\n",
    "train_agent(env, agent, main_weight_folder=weight_dir,\n",
    "            n_episodes=1200, evaluation_freq=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test your agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can test your weights. Hint: The weights are only saved when the agent reaches an average of 0.5+ points. If you want to test the pre-trained weights skip to Section 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_weights(main_folder=weight_dir)\n",
    "evaluate_current_weights(env, agent, env.num_agents, train_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test pre-trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_cfg = []\n",
    "config = default_cfg()\n",
    "config.multi_agent_actions = env.action_sizes\n",
    "config.multi_agent_states = env.state_sizes\n",
    "for i in range(env.num_agents):\n",
    "    agent_cfg = copy.deepcopy(config)\n",
    "    agent_cfg.state_size = config.multi_agent_states[i]\n",
    "    agent_cfg.action_size = config.multi_agent_actions[i]\n",
    "    maddpg_cfg = MADDDPGConfig()\n",
    "    maddpg_cfg.subpolicy_configs = [agent_cfg] * NUM_SUB_POLICIES\n",
    "    multi_cfg.append(maddpg_cfg)\n",
    "\n",
    "agent = MADDPGManager(maddpg_agents_configs=multi_cfg, update_every=config.update_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_weights(main_folder=\"best_weights/\")\n",
    "evaluate_current_weights(env, agent, env.num_agents, train_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
